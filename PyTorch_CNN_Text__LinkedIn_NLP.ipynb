{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bballdave025/nlp_w_pytorch_zhongyu-pan/blob/main/PyTorch_CNN_Text___LinkedIn_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55KVBFK78Iag"
   },
   "source": [
    "# Convolutional Neural Network for Text Classification Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaIL7xmHVEuX"
   },
   "source": [
    "[Go straight to the code](#Installs-and-Imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU93StT09nAE"
   },
   "source": [
    "## Navigation - Get the Notebook from Different Places for Different Uses\n",
    "\n",
    "### (Navigation for the main presentation notebook)\n",
    "\n",
    "[Google CoLab on my Google Drive](https://colab.research.google.com/drive/1PKkdbNcqUfV0sHCosWZf3JdF6F3kGoj7?usp=sharing) - A place to see all inputs\n",
    "and outputs for the notebook, though you can't edit it without re-saving it.\n",
    "\n",
    "<br/>\n",
    "\n",
    "[GitHub Repo (link to be put in, soon)](https://github.com/bballdave025/nlp_w_pytorch_zhongyu-pan/) - Code repository: a place to see the latest changes as well as the Jupyter Notebooks completed earlier\n",
    "\n",
    "<br/>\n",
    "\n",
    "[GitHub Notebook File (link to be put in, soon)](#) - I don't think this\n",
    "is as useful as the repo, but you can see the IPYNB file placeholder.\n",
    "This file will only have input - I scrub the output before committing\n",
    "any updates, because it's easier to do `diff`s (see changes in code)\n",
    "on Jupyter Notebooks when you don't have the outputs.\n",
    "\n",
    "<br/>\n",
    "\n",
    "[On MyBinder (link to be put in, soon)](#) - A place to interact with the notebook, where you'll be led to the notebook without output and can\n",
    "run the code and see the results yourself.<br/>\n",
    "A note, [MyBinder](https://mybinder.org) is a great online project which allows you to interactively run a Jupyter notebook completely online. It's nice to have when you'd like to play with code and better see the outputs that come from running that code. I've had some problems with images going down, but I'm going to work to keep this one up and running for access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVzAxwDI7vJB"
   },
   "source": [
    "## Putting Together All the Work from the Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzW07eEC9VzV"
   },
   "source": [
    "Course For NLP from LinkedIn\n",
    "\n",
    "https://www.linkedin.com/learning/natural-language-processing-with-pytorch\n",
    "\n",
    "The teacher is Zhonyu Pan, Content Creator at LinkedIn\n",
    "\n",
    "We use PyTorch and a Convolutional Neural Network (using NLP features\n",
    "rather than the pixel position features we use with image processing) to\n",
    "do our text classification.\n",
    "\n",
    "`Input -> Convolution -> Pooling -> ... -> Fully-connected layer -> Output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMXpg60q9jcV"
   },
   "source": [
    "We are also learning about RNNs. RNN doesn't only pass data forward, but also feeds the data back into itself. CNN only goes forward. RNN can remember context before and after words in a sequence. It's usually slower that a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl1dzqUi8R4n"
   },
   "source": [
    "### Installs and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHGQJEZz8_v4"
   },
   "source": [
    "#### Robust Install-if-Needed Code\n",
    "\n",
    "(I don't want to have to mess with whether the runtime has been\n",
    "disconnected or if the version is right or whatever. I'm ensuring\n",
    "compatibility for CoLab here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ3BoL6DJPX-"
   },
   "source": [
    "<b>Functions</b>\n",
    "\n",
    "I'm just going to make something simple that makes sure I install the\n",
    "packages I need if they haven't already been installed. This will be\n",
    "especially useful for CoLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zVlbJxoK9JaS"
   },
   "outputs": [],
   "source": [
    "ilmd_imported = False\n",
    "\n",
    "try:\n",
    "  import importlib_metadata\n",
    "  ilmd_imported = True\n",
    "except ImportError:\n",
    "  !pip install \"importlib-metadata==8.4.0\"\n",
    "finally:\n",
    "  if not ilmd_imported:\n",
    "    import importlib_metadata\n",
    "  ##endof:  if not ilmd_imported\n",
    "##endof:  try/except/finally <importlib_metadata>\n",
    "\n",
    "def is_package_installed(package_name):\n",
    "  try:\n",
    "    # <find-the-package-in-the-list>\n",
    "    dist = importlib_metadata.distribution(package_name)\n",
    "    print(f\"{package_name} {dist.version} is installed.\")\n",
    "    return True\n",
    "  except importlib_metadata.PackageNotFoundError:\n",
    "    print(f\"{package_name} is not installed.\")\n",
    "    return False\n",
    "  finally:\n",
    "    pass\n",
    "  ##endof:  try/except/finally <find-the-package-in-the-list>\n",
    "##endof:  def check_package_installed(package_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0mfq5kKXSjQ",
    "outputId": "bc3f9cef-af82-42e0-c38c-b6680cce528a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05T224706+0000\n"
     ]
    }
   ],
   "source": [
    "# # I won't need this anymore\n",
    "#!date -u +\"%Y-%m-%dT%H%M%S%z\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXBph8YvXwKd"
   },
   "source": [
    "Output was\n",
    "\n",
    "`2024-09-05T224706+0000`\n",
    "\n",
    "... \\[versions that work ... as of ...\\] 2024-09-05 at 22:47:06 UTC+0000n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thiGiJugAuoy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVCjmZdiwUzb"
   },
   "source": [
    "**Preprocessing text dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOgPHnQtAutO",
    "outputId": "0a1bd2a1-bcb4-4f12-b055-f27162d5f23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 966\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDVEivhqAuxA"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', lower=True)\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxiaBGivAu3K",
    "outputId": "d901b255-6c14-47a7-d232-bfbcbe5fbea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train_5500.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336k/336k [00:00<00:00, 3.04MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading TREC_10.label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23.4k/23.4k [00:00<00:00, 881kB/s]\n"
     ]
    }
   ],
   "source": [
    "train, test = datasets.TREC.splits(TEXT, LABEL)\n",
    "train, val = train.split(random_state = random.seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kxky-ODaRh0U",
    "outputId": "d5ac26d8-a58d-4060-8ff5-91275ecb6dc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'ENTY', 'text': ['how', 'do', 'you', 'say', '2', 'in', 'latin', '?']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoM5PVpl5Kra"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKAd1xen2o5b",
    "outputId": "b6e86d3b-76be-48b5-d112-37e3e8efe007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of TEXT: 2641\n",
      "Vocabulary size of LABEL: 6\n",
      "defaultdict(None, {'ENTY': 0, 'HUM': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
    "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEiW7Ri9S-el"
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test),\n",
    "    batch_size = 64,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hieTuqmvwieC"
   },
   "source": [
    "**Building a Simple CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfXWPHz3S-hR"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, vocabulary_size, embedding_size,\n",
    "               kernels_number, kernel_sizes, output_size, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "    self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=kernels_number, kernel_size=(k, embedding_size))\n",
    "                                            for k in kernel_sizes])\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fully_connected = nn.Linear(len(kernel_sizes) * kernels_number, output_size)\n",
    "  def forward(self, text):\n",
    "    text = text.permute(1, 0)\n",
    "    input_embeddings = self.embedding(text)\n",
    "    input_embeddings = input_embeddings.unsqueeze(1)\n",
    "    conved = [F.relu(convolution_layer(input_embeddings)).squeeze(3) for convolution_layer in self.convolution_layers]\n",
    "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "    concat = self.dropout(torch.cat(pooled, dim=1))\n",
    "    final_output = self.fully_connected(concat)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCURSmjUS-kV"
   },
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "embedding_size = 100\n",
    "kernels_number = 100\n",
    "kernel_sizes = [2, 3, 4]\n",
    "output_size = len(LABEL.vocab)\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOcYKu_SmQvy"
   },
   "outputs": [],
   "source": [
    "model = CNN(input_size, embedding_size, kernels_number, kernel_sizes, output_size, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMjhG8IOmhZf",
    "outputId": "662a55ce-768f-4768-81ee-25caf8651911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding): Embedding(2641, 100)\n",
      "  (convolution_layers): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fully_connected): Linear(in_features=300, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKcm7ZJQm9Oe",
    "outputId": "fbda9754-d089-4a3b-d956-1092b570db94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(2641, 100)\n",
       "  (convolution_layers): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fully_connected): Linear(in_features=300, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMoLI86g0FA_"
   },
   "source": [
    "**Train and Evaluate Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUYzZfU9nBdP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk5boWJWnld4"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, actual_label):\n",
    "    max_predictions = predictions.argmax(dim = 1, keepdim = True, )\n",
    "    correct_predictions = max_predictions.squeeze(1).eq(actual_label)\n",
    "    accuracy = correct_predictions.sum() / torch.cuda.FloatTensor([actual_label.shape[0]])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynHqY1ysoT57"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.text)\n",
    "\n",
    "        loss = criterion(predictions, batch.label)\n",
    "\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoIQYPOJo164"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CF2U8pZBq1t"
   },
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Jd1eYGupW0x",
    "outputId": "ca13c6fe-59d4-41a6-a657-0e3323e7997b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "\tTrain Loss: 1.312 | Train Acc: 48.76%\n",
      "\t Validation Loss: 0.952 |  Validation Acc: 64.50%\n",
      "Epoch 2 \n",
      "\tTrain Loss: 0.779 | Train Acc: 71.66%\n",
      "\t Validation Loss: 0.744 |  Validation Acc: 72.32%\n",
      "Epoch 3 \n",
      "\tTrain Loss: 0.585 | Train Acc: 81.01%\n",
      "\t Validation Loss: 0.654 |  Validation Acc: 74.99%\n",
      "Epoch 4 \n",
      "\tTrain Loss: 0.452 | Train Acc: 85.48%\n",
      "\t Validation Loss: 0.628 |  Validation Acc: 76.36%\n",
      "Epoch 5 \n",
      "\tTrain Loss: 0.359 | Train Acc: 88.29%\n",
      "\t Validation Loss: 0.583 |  Validation Acc: 78.56%\n",
      "Epoch 6 \n",
      "\tTrain Loss: 0.275 | Train Acc: 91.91%\n",
      "\t Validation Loss: 0.548 |  Validation Acc: 80.06%\n",
      "Epoch 7 \n",
      "\tTrain Loss: 0.207 | Train Acc: 94.12%\n",
      "\t Validation Loss: 0.550 |  Validation Acc: 80.89%\n",
      "Epoch 8 \n",
      "\tTrain Loss: 0.163 | Train Acc: 95.83%\n",
      "\t Validation Loss: 0.544 |  Validation Acc: 81.54%\n",
      "Epoch 9 \n",
      "\tTrain Loss: 0.137 | Train Acc: 96.33%\n",
      "\t Validation Loss: 0.530 |  Validation Acc: 81.62%\n",
      "Epoch 10 \n",
      "\tTrain Loss: 0.104 | Train Acc: 97.80%\n",
      "\t Validation Loss: 0.542 |  Validation Acc: 80.82%\n",
      "Epoch 11 \n",
      "\tTrain Loss: 0.090 | Train Acc: 98.02%\n",
      "\t Validation Loss: 0.546 |  Validation Acc: 81.08%\n",
      "Epoch 12 \n",
      "\tTrain Loss: 0.076 | Train Acc: 98.55%\n",
      "\t Validation Loss: 0.545 |  Validation Acc: 81.60%\n",
      "Epoch 13 \n",
      "\tTrain Loss: 0.066 | Train Acc: 98.47%\n",
      "\t Validation Loss: 0.548 |  Validation Acc: 81.96%\n",
      "Epoch 14 \n",
      "\tTrain Loss: 0.054 | Train Acc: 98.93%\n",
      "\t Validation Loss: 0.554 |  Validation Acc: 82.38%\n",
      "Epoch 15 \n",
      "\tTrain Loss: 0.048 | Train Acc: 99.09%\n",
      "\t Validation Loss: 0.557 |  Validation Acc: 81.67%\n",
      "Epoch 16 \n",
      "\tTrain Loss: 0.042 | Train Acc: 98.96%\n",
      "\t Validation Loss: 0.564 |  Validation Acc: 82.26%\n",
      "Epoch 17 \n",
      "\tTrain Loss: 0.038 | Train Acc: 99.28%\n",
      "\t Validation Loss: 0.562 |  Validation Acc: 82.45%\n",
      "Epoch 18 \n",
      "\tTrain Loss: 0.034 | Train Acc: 99.40%\n",
      "\t Validation Loss: 0.578 |  Validation Acc: 82.45%\n",
      "Epoch 19 \n",
      "\tTrain Loss: 0.030 | Train Acc: 99.38%\n",
      "\t Validation Loss: 0.599 |  Validation Acc: 82.44%\n",
      "Epoch 20 \n",
      "\tTrain Loss: 0.030 | Train Acc: 99.30%\n",
      "\t Validation Loss: 0.590 |  Validation Acc: 82.47%\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "best_acc = float('-inf')\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    # Write the code here\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    # Write the code here\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_acc > best_acc:\n",
    "        # Write the code here\n",
    "        best_acc = valid_acc\n",
    "        torch.save(model.state_dict(), 'trec.pt')\n",
    "\n",
    "    print(f'Epoch {epoch+1} ')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAcltNzgqgDb",
    "outputId": "b94b1cde-8498-40ce-9123-68653d68be0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.416 | Test Acc: 87.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trec.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcZeq-C33vBk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
